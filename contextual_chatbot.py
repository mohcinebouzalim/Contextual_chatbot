# -*- coding: utf-8 -*-
"""Contextual_chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZEBzGG6hze6xZsYRjA_Ol3ee7RzZdUHN
"""

import nltk
nltk.download("punkt")

from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()

from nltk.corpus import stopwords
nltk.download('stopwords')

import tensorflow as tf
import numpy as np 
import tflearn
import json
import random

from google.colab import files
files.upload()

with open("intents.json") as json_data:
  intents = json.load(json_data)

words = []
classes = []
documents = []

for intent in intents["intents"]:
  for pattern in intent["patterns"]:
    w = nltk.word_tokenize(pattern)
    words.extend(w)
    documents.append((w, intent["tag"]))
    if intent["tag"] not in classes:
      classes.append(intent["tag"])

stop_words = set(stopwords.words("english"))
punctuations = ["?", "!", ",", ".", ":"]
words = [stemmer.stem(w.lower()) for w in words if w not in stop_words]
words = [w for w in words if w not in punctuations]
words = [w for w in words if len(w)>1]
words = sorted(list(set(words)))
classes = sorted(list(set(classes)))

training = []
output = []
output_empty = [0]*len(classes)

for doc in documents:
  bow = []
  pattern_words = doc[0]
  pattern_words = [stemmer.stem(w.lower()) for w in pattern_words]
  for w in words:
    bow.append(1) if w in pattern_words else bow.append(0)

  output_row = list(output_empty)
  output_row[classes.index(doc[1])] = 1 

  training.append([bow, output_row])

training = np.array(training)

train_x = list(training[:,0])
train_y = list(training[:,1])

tf.reset_default_graph()

net = tflearn.input_data(shape=[None, len(train_x[0])])
net = tflearn.fully_connected(net, 10)
net = tflearn.fully_connected(net, 10)
net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')
net = tflearn.regression(net)

model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')

model.fit(train_x, train_y, n_epoch=800, batch_size=10, show_metric=True)
model.save('model.tflearn')

def clean_sentence(sentence):
  sentence_words = nltk.word_tokenize(sentence)
  sentence_words = [stemmer.stem(w.lower()) for w in sentence_words]
  return sentence_words

def bow(sentence, words):
  sentence_words = clean_sentence(sentence)
  bag = [0]*len(words)
  for s in sentence_words:
    for i,w in enumerate(words):
      if w==s:
        bag[i] = 1
  return(np.array(bag))

def classify(sentence):
  results = model.predict([bow(sentence, words)])[0]
  results = [[i, r] for i, r in enumerate(results) if r>0.3]
  results.sort(key=lambda x: x[1], reverse=True)
  return_list = []
  for r in results:
    return_list.append((classes[r[0]], r[1]))
  return return_list

def response(sentence):
  results = classify(sentence)
  if results:
    while results:
      for intent in intents["intents"]:
        if intent["tag"] == results[0][0]:
          print(random.choice(intent["responses"]))
      results.pop(0)